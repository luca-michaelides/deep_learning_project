{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luca-michaelides/deep_learning_project/blob/main/llama_chats/chat_llama2_smoothllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5300fa",
      "metadata": {
        "id": "ea5300fa"
      },
      "source": [
        "## Initial test with Llama2 LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gc, time, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "try:\n",
        "    del model\n",
        "except NameError:\n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_token)\n",
        "\n",
        "print(\"Loading 4-bit quantized model (this may take 60-120s)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded. Sample param device:\", next(model.parameters()).device)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "prompt = \"<s>[INST] <<SYS>>\\nYou are a helpful assistant.\\n<</SYS>>\\n\\nSay hello briefly. [/INST]\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs.pop(\"token_type_ids\", None)\n",
        "device = next(model.parameters()).device\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "t0 = time.time()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=24,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "t1 = time.time()\n",
        "print(\"Generation time: %.2fs\" % (t1 - t0))\n",
        "print(\"Decoded output:\\n\", tokenizer.decode(out[0], skip_special_tokens=True))\n",
        "\n",
        "print(\"\\n--- nvidia-smi ---\")\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SA5H2Zt6H5WV",
        "outputId": "411e5b68-379b-4483-e225-9a4561e38e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379,
          "referenced_widgets": [
            "2951b21c969e49688fa56e95d122127c",
            "67b755f03879460094d79cc90905098f",
            "68e05cac315d4d36984c7162c5e91507",
            "22e94cd382d3412f8398b533f0e898da",
            "80e0e0d537b542eca85cc4617b6caf58",
            "8fa308e5b735478192ff10ff78fc845a",
            "fca47dcc987b4037b2c26c89b27fbfdd",
            "e86c6a2b7b79427681d17a087ad7a87c",
            "eff4d9ebabc6423588a409782017d1bd",
            "b8fec5bb74d6494189a0f0086bdf8e94",
            "88c58668c7c44f2d9d29a08c19f4adb6"
          ]
        }
      },
      "id": "SA5H2Zt6H5WV",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLoading tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 4-bit quantized model (this may take 60-120s)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2951b21c969e49688fa56e95d122127c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded. Sample param device: cuda:0\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Generation time: 2.37s\n",
            "Decoded output:\n",
            " [INST] <<SYS>>\n",
            "You are a helpful assistant.\n",
            "<</SYS>>\n",
            "\n",
            "Say hello briefly. [/INST]  Hello! It's nice to meet you! How can I assist you today?\n",
            "\n",
            "--- nvidia-smi ---\n",
            "name, memory.total [MiB], memory.used [MiB]\n",
            "Tesla T4, 15360 MiB, 6568 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "def format_llama2_chat_prompt(user_message, system_message=\"You are a helpful assistant.\"):\n",
        "    return (\n",
        "        \"<s>[INST] <<SYS>>\\n\"\n",
        "        f\"{system_message}\\n\"\n",
        "        \"<</SYS>>\\n\\n\"\n",
        "        f\"{user_message} [/INST]\"\n",
        "    )\n",
        "\n",
        "import time, torch\n",
        "\n",
        "# find device where most params live (works for sharded models)\n",
        "_model_device = next(model.parameters()).device\n",
        "\n",
        "def chat(user_input, max_tokens=64, temperature=0.1, do_sample=None):\n",
        "    if do_sample is None:\n",
        "        do_sample = (temperature > 0)\n",
        "\n",
        "    prompt = format_llama2_chat_prompt(user_input)  # your formatting function\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs.pop(\"token_type_ids\", None)\n",
        "\n",
        "    # move inputs to model device (handles sharded / auto device_map)\n",
        "    inputs = {k: v.to(_model_device) for k, v in inputs.items()}\n",
        "\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=do_sample,\n",
        "            temperature=temperature if do_sample else 0.0,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True\n",
        "        )\n",
        "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "    dt = time.time() - t0\n",
        "\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    # The model returns prompt+response; return only the part after the [/INST] marker if you prefer:\n",
        "    # split on the closing tag and keep the tail:\n",
        "    tail = decoded.split(\"[/INST]\")[-1].strip()\n",
        "    print(tail)\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "HNq_7M0nDD_e"
      },
      "id": "HNq_7M0nDD_e",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"Answer with 1 word only. What is the capital city of Spain?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Bl-D0SDpeT",
        "outputId": "b063721f-61d8-46a2-8505-417914964eee"
      },
      "id": "h7Bl-D0SDpeT",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Madrid\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2951b21c969e49688fa56e95d122127c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67b755f03879460094d79cc90905098f",
              "IPY_MODEL_68e05cac315d4d36984c7162c5e91507",
              "IPY_MODEL_22e94cd382d3412f8398b533f0e898da"
            ],
            "layout": "IPY_MODEL_80e0e0d537b542eca85cc4617b6caf58"
          }
        },
        "67b755f03879460094d79cc90905098f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fa308e5b735478192ff10ff78fc845a",
            "placeholder": "​",
            "style": "IPY_MODEL_fca47dcc987b4037b2c26c89b27fbfdd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "68e05cac315d4d36984c7162c5e91507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e86c6a2b7b79427681d17a087ad7a87c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eff4d9ebabc6423588a409782017d1bd",
            "value": 2
          }
        },
        "22e94cd382d3412f8398b533f0e898da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8fec5bb74d6494189a0f0086bdf8e94",
            "placeholder": "​",
            "style": "IPY_MODEL_88c58668c7c44f2d9d29a08c19f4adb6",
            "value": " 2/2 [01:00&lt;00:00, 27.88s/it]"
          }
        },
        "80e0e0d537b542eca85cc4617b6caf58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa308e5b735478192ff10ff78fc845a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fca47dcc987b4037b2c26c89b27fbfdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e86c6a2b7b79427681d17a087ad7a87c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eff4d9ebabc6423588a409782017d1bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8fec5bb74d6494189a0f0086bdf8e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88c58668c7c44f2d9d29a08c19f4adb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}